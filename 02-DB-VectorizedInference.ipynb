{"cells":[{"cell_type":"markdown","source":["##Vectorized PandasUDF + Keras for Inference\n\nOne of the exciting things we can do with Arrow/Vectorized PandasUDF is efficiently integrate Spark with numeric or even GPU code that was *not* designed with Spark in mind.\n\nFor example, we might have a model that we've build with Keras+TensorFlow -- without Spark in the picture at all -- and then efficiently use that model to perform inference on big datasets with Spark.\n\nIn this module, we'll do just that: we'll train a simple neural network using Keras, save it to disk, and then use it from Spark via PandasUDF."],"metadata":{}},{"cell_type":"markdown","source":["We'll start by using Pandas to read our Diamonds dataset, and to save time (and featurization) we'll just use the 6 continuous variables in our model to predict price."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport IPython.display as disp\n\ninput_file = \"/dbfs/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n\ndf = pd.read_csv(input_file, header = 0)\ndf.drop(df.columns[0], axis=1, inplace=True)\ndf.drop(df.columns[1:4], axis=1, inplace=True)\ndisp.display(df)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["We'll do a train/test split, and look at a few rows as a sanity check:"],"metadata":{}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n\nX = df.drop(df.columns[3], axis=1)\ny = df.iloc[:,3:4]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint(X[:5])\n\nprint(y[:5])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Now we'll build a simple feed-forward perceptron network in Keras, and train it for a minute or so, then check our performance"],"metadata":{}},{"cell_type":"code","source":["dbutils.library.installPyPI(\"tensorflow\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["import tensorflow as tf\nimport numpy as np\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(16, input_dim=6, kernel_initializer='normal', activation='relu')) \nmodel.add(tf.keras.layers.Dense(1, kernel_initializer='normal', activation='linear'))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\nmodel.fit(X_train, y_train, epochs=64, batch_size=128, validation_split=0.1, verbose=2)\n\nscores = model.evaluate(X_test, y_test)\nprint(\"\\nroot %s: %f\" % (model.metrics_names[1], np.sqrt(scores[1])))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["model.save(\"/tmp/model\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sh cp /tmp/model /dbfs/tmp/model"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Ok, now let's get Spark DataFrame with our test data. In real life, we'd read this directly from S3, HDFS, Kafka, or wherever.\n\nBut since this test set is already on the driver (and not very big), we can make a distributed Spark DF from the Pandas DF."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\ntestDF = spark.createDataFrame(pd.DataFrame(X_test, columns=[\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]))\n\ndisplay(testDF)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["When our data shows up, we'll have to reshape it a little bit, and then we can do a regular keras `model.predict()` on it. A  raw prediction looks like this:"],"metadata":{}},{"cell_type":"code","source":["model.predict(X_test[:5])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["That's almost perfect ... but Spark is going to expect a flat array of outputs from the PandasUDF (one for each input)"],"metadata":{}},{"cell_type":"code","source":["pd.Series(model.predict(X_test[:5]).flatten())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Hopefully with that out of the way, the following implementation (which uses those reshape tricks above) will work.\n\n*Note: in real life, don't load the model on each call ... make sure the model is available ahead of time on the executors and just loaded once (e.g. by distributing a module/pyfile)*"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.functions import PandasUDFType\n\nimport numpy as np\n\n@pandas_udf(\"double\", PandasUDFType.SCALAR)\ndef keras_predict(*v):\n  # reshape to records x len(v) columns\n  reshaped = np.asarray(v).reshape(len(v), -1).transpose()\n  keras_model = tf.keras.models.load_model('/tmp/model')\n  return pd.Series(keras_model.predict(reshaped).flatten())    "],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["And now we can `keras_predict` on the columns in the Spark DataFrame:"],"metadata":{}},{"cell_type":"code","source":["testDF.select(keras_predict(*testDF.columns)).show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["And now ... we've got Spark and all of our Python goodness playing nice!"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"02-DB-VectorizedInference","notebookId":3829983292049288},"nbformat":4,"nbformat_minor":0}
