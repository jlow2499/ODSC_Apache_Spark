{"cells":[{"cell_type":"markdown","source":["# Spark + Distributed Deep Learning Training\n<br>\n* The Challenge\n* Outside of Spark\n* With Spark\n* Transfer Learning"],"metadata":{}},{"cell_type":"markdown","source":["### The Challenge\n\n* Multiple modes of distributed DL training\n  * Sync vs Async SGD, Parameter server?, AllReduce?\n  * How much flexibility is the \"right\" amount to expose?\n* Scheduling and cluster resources\n  * Reserving GPUs\n  * Placing tasks\n  * Timing: Spark scheduler model vs MPI model\n  \n### The Upshot\n\n1. It's easy(ish) to do a brute-force training using the old Spark scheduling approach, but that's not performant.\n2. State-of-the art, fast training mechanisms (e.g., Horovod) need different cluster and scheduling considerations.\n3. In the past, these integrations have been dicey...\n4. Databricks' MLRuntime and HorovodEstimator serve as a proof-of-concept for a nice future solution"],"metadata":{}},{"cell_type":"markdown","source":["### Outside of Spark\n\nWha? For now, it's actually the most straightforward path!\n\nA workflow orchestrator like Airflow can help you sequence\n* ETL/Featurization (Spark) work \n* Horovod-based distirbuted DL training\n* Resume Spark data processing pipeline\n\nIt's not perfect ... there can be problems ensuring the proper cluster resources are available at the right time, while maintaining high utilization."],"metadata":{}},{"cell_type":"markdown","source":["### Options For Training Full Models under Spark's Control/Scheduling\n\n__Present__\n* Intel BigDL\n  * CPU focus\n  * Leverages Xeon Phi / Skylake coprocessors\n  * Some performance questions\n  * May be easier for enterprises to buy/provision (as compared to NVidia datacenter GPUs)\n* DeepLearning4J\n  * JVM based, Spark integration\n  * GPU support\n* TensorFlowOnSpark\n  * CPU/GPU/Infiniband support\n  * Doesnt' really integrate with Spark APIs/Patterns\n* Microsoft MMLSpark (OSS)\n  * Leverages CNTK\n  * MSFT Research distributed training: https://arxiv.org/pdf/1804.04031.pdf\n  \n* A number of others with smaller communities or little activity\n* Databricks MLRuntime + TensorFlow + Horovod\n  * https://docs.databricks.com/applications/deep-learning/distributed-deep-learning/horovod-estimator.html\n\n__Future__\n* &#x1f44d; __*Future (2019): Apache Spark (Barrier Mode, MPI) + Horovod + TensorFlow*__\n  * Open-source Spark (Project Hydrogen) is addressing this: https://www.youtube.com/watch?v=vVZwzG7uKvI\n* Related work includes\n  * Alchemist (UCBerkeley), with presentations here at ODSC https://arxiv.org/abs/1806.01270\n  * Spark-MPI (Intel/Brookhaven) https://arxiv.org/pdf/1806.01110.pdf"],"metadata":{}},{"cell_type":"markdown","source":["__Transfer Learning__ \n\nTransfer learning involves using an existing, already-trained neural network as part of a new model.\n\nFor example, it is common to use large chunks of pre-trained image recognition networks as \"feature extractors\" for new image recognition tasks.\n\nAny of the listed tools -- or your own Python-based model, used as a pipeline step (Transformer) with a vectorized PandasUDF call -- can help you implement transfer learning patterns.\n\nIn addition, the following tools have APIs and examples designed to simplify this process:\n\n* Databricks - Spark Deep Learning Pipelines (OSS)\n* Microsoft - MMLSpark"],"metadata":{}},{"cell_type":"markdown","source":["# Wrapup\n\n## Q & A"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"06-DistDL-Wrapup","notebookId":3829983292049254},"nbformat":4,"nbformat_minor":0}
